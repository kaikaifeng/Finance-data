\\yarn-site.xml
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>192.168.111.1</value>
	<describe>For each rm-id, specify the hostname the RM corresponds to. Alternately, one could set each of the RM’s service addresses. </describe>
</property>

<property>
    	<name>yarn.resourcemanager.resource-tracker.address</name>
     	<value>192.168.111.1:8025</value>
	<describe>NodeManager连接ResourceManager的地址<describe>
	<describe>For each rm-id, specify host:port for NodeManagers to connect. If set, overrides the hostname set in yarn.resourcemanager.hostname.rm-id. </describe>
</property>

<property>
      	<name>yarn.resourcemanager.scheduler.address</name>
      	<value>192.168.111.1:8030</value>
	<describe>ApplicationMasters从ResourceManager获取资源的地址<describe>
	<describe>For each rm-id, specify scheduler host:port for ApplicationMasters to obtain resources. If set, overrides the hostname set in yarn.resourcemanager.hostname.rm-id. </describe>
</property>

<property>
      	<name>yarn.resourcemanager.scheduler.class</name>
      	<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
	<describe>启用的资源调度器主类，比较重要的可能是Capacity Scheduler和Fair Scheduler
	CapacityScheduler的特点：
	资源按比例分配给各个队列，并添加各种严格的限制防止个别用户或者队列独占资源
	不支持负载均衡
	Container请求资源粒度为最小资源量的整数倍（粒度粗）
	更多描述：http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html
	</describe>
</property>

<property>
      	<name>yarn.resourcemanager.address</name>
      	<value>192.168.111.1:8032</value>
	<describe>客户端通过这个地址提交任务</describe>
	<describe>For each rm-id, specify host:port for clients to submit jobs. If set, overrides the hostname set in yarn.resourcemanager.hostname.rm-id. </describe>
</property>

<property>
       	<name>yarn.nodemanager.local-dirs</name>
       	<value>${hadoop.tmp.dir}/nodemanager/local</value>
	<describe>中间结果存放位置，类似于1.0中的mapred.local.dir。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
       	<name>yarn.nodemanager.address</name>
       	<value>0.0.0.0:8034</value>
	<describe>用于重启</describe>
	<describe>Ephemeral ports (port 0, which is default) cannot be used for the NodeManager’s RPC server specified via yarn.nodemanager.address as it can make NM use different ports before and after a restart. This will break any previously running clients that were communicating with the NM before restart. Explicitly setting yarn.nodemanager.address to an address with specific port number (for e.g 0.0.0.0:45454) is a precondition for enabling NM restart. </describe>
</property>

<property>
       	<name>yarn.nodemanager.remote-app-log-dir</name>
       	<value>${hadoop.tmp.dir}/nodemanager/remote</value>
	<describe>当应用程序运行结束后，日志被转移到的HDFS目录（启用日志聚集功能时有效）</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
       	<name>yarn.nodemanager.log-dirs</name>
       	<value>${hadoop.tmp.dir}/nodemanager/logs</value>
	<describe>日志存放地址（可配置多个目录）</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
	<describe>NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	<describe>运行mapreduce需要的类？？？</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>192.168.111.1:18088</value>
	<describe>ResourceManager对外web ui地址。用户可通过该地址在浏览器中查看集群各类信息</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>192.168.111.1:18141</value>
	<describe>ResourceManager对管理员暴露的访问地址。管理员通过该地址向RM发送管理命令等。</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
	<describe>是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
	<describe>虚拟内存检查，默认为true</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>40960</value>
	<describe>声明的NodeManager总的可用物理内存，注意，是“声明的”，即即使实际没有这么多可用内存，也会按照这个数值使用，web ui中显示的40GB的来源
	</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>mapred.tasktracker.map.tasks.maximum</name>
        <value>30</value>
	<describe>
	单台机器上并发的map任务数量，据称设置为cpu数目或者cpu数目 - 1最佳
	hadoop2中以yarn.nodemanager.resource.vcore替代
	</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>256</value>
	<describe>单个任务可申请的最小内存资源量</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>8192</value>
	<describe>单个任务可申请的最大内存资源量</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
	<describe>任务每使用1MB物理内存，最多可使用虚拟内存量，默认是2.1</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
	<name>yarn.scheduler.maximum-allocation-vcores</name>
	<describe>RM可以分配给一个容器的最多虚拟CPU个数，默认为32</describe>
</property>

\\hdfs-site.xml
<property>
        <name>dfs.replication</name>
        <value>2</value>
	<describe>上传到hdfs的文件创建的副本数量</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/hadoop/hdfs/name</value>
        <final>true</final>
	<describe>NameNode上存放元数据的目录</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/hadoop/hdfs/data</value>
        <final>true</final>
	<describe>真正的datanode数据保存路径，可以写多块硬盘，逗号分隔.把这些位置分散在每个节点上的所有磁盘上可以实现磁盘 I/O 平衡，因此会显著改进磁盘 I/O 性能。</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>dfs.datanode.max.xcievers</name>
        <value>300000</value>
	<describe>相当于linux下的打开文件最大数量，文档中无此参数，当出现DataXceiver报错的时候，需要调大。默认256</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/tmp</value>
	<describe>There're three HDFS properties which contain hadoop.tmp.dir in their values dfs.name.dir: directory where namenode stores its metadata, with default value ${hadoop.tmp.dir}/dfs/name. dfs.data.dir: directory where HDFS data blocks are stored, with default value ${hadoop.tmp.dir}/dfs/data.fs.checkpoint.dir: directory where secondary namenode store its checkpoints, default value is ${hadoop.tmp.dir}/dfs/namesecondary.This is why you saw the /mnt/hadoop-tmp/hadoop-${user.name} in your HDFS after formatting namenode.</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
	<name>dfs.http.address</name>
	<value>hadoop1:50070</value>
	<description>
	The address and the base port where the dfs namenode web ui will listen on.
	If the port is 0 then the server will start on a free port.
	</description>
	<describe>hdfs的web ui地址</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
	<name>dfs.namenode.secondary.http-address</name>
	<value>hadoop2:50090</value>
	<describe>Secondary NameNode的web ui地址</describe>
	<describe>Haven't found the original description</describe>
</property>

\\core-site.xml
<property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/tmp</value>
	<describe>临时文件夹，指定后需将使用到的所有子级文件夹都要手动创建出来，否则无法正常启动服务（这里是正确的配置位置）</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>fs.defaultFS</name>
        <value>hdfs://192.168.111.1:8020</value>
	<describe>hadoop访问目录节点nameNode的地址</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>192.168.111.1</value>
	<describe>该参数表示可以通过httpfs接口hdfs的ip地址限制（应该没特别大作用）</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
	<describe>通过httpfs接口访问的用户获得的群组身份在两个参数中，第三位（root）的参数表示启动hadoop集群的用户。</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
	<describe>用作序列化文件处理时读写buffer的大小</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>hadoop.proxyuser.hduser.hosts</name>
        <value>*</value>
	<describe>见上</describe>
	<describe>Haven't found the original description</describe>
</property>
<property>
        <name>hadoop.proxyuser.hduser.groups</name>
        <value>*</value>
	<describe>见上</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>fs.checkpoint.period</name>
        <value>3600</value>
        <description>The number of seconds between two periodic checkpoints.
        </description>
	<describe>用来控制SecondNameNode的checkpoint时间间隔的。
如果距离上次checkpoint的时间大于fs.checkpoint.period参数所指定的时间，那么就会触发checkpoint事件。
SecondNameNode会将当前最新的fsimage数据归档到previous.checkpoint文件夹中</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>fs.checkpoint.size</name>
        <value>67108864</value>
	<describe>
	以日志大小间隔做备份间隔，只对SNN生效
	git commit: AMBARI-4437. fs.checkpoint.size is deprecated in HDP2, should be replaced by dfs.namenode.checkpoint.txns
	</describe>
	<describe>Haven't found the original description</describe>
</property>

\\mapred-site.xml
<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
	<describe>MapReduce运行在yarn集群上</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>mapreduce.jobhistory.address</name>
        <value>192.168.111.1:10020</value>
	<describe>Hadoop历史作业情况服务需要的地址</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>192.168.111.1:19888</value>
	<describe>Hadoop历史作业情况web ui地址</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>mapred.child.java.opts</name>
        <value>-Xmx8192m</value>
	<describe>
	这个参数是配置每个map或reduce使用的内存数量，默认是200m，一般情况下，该值设置为 总内存/并发数量(=核数)
	最近发现Hadoop Job集群的load非常高，最后发现是mapred.child.Java.opts设置过大导致的，我们当初设置为-Xmx5120导致TaskTracker上内存资源耗尽，进而开始不断swap磁盘上数据
	代替以mapred.map.child.java.opts和mapred.reduce.child.java.opts
	</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>mapreduce.map.memory.mb</name>
        <value>4096</value>
	<describe>
	每个Map Task需要的内存量
	mapreduce.map.memory.mb是向RM申请的内存资源大小，这些资源可用用于各种程序语言编写的程序, mapred.map.child.java.opts一般只用于配置JVM参数
	</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>4096</value>
	<describe>每个Reduce Task需要的内存量;是逻辑值，用于监控</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>mapreduce.map.java.opts</name>
        <value>-Xmx6g</value>
	<describe>
	JVM进程堆的大小
	因为在yarn container这种模式下，JVM进程跑在container中，mapreduce.{map|reduce}.java.opts能够通过Xmx设置JVM最大的heap的使用，一般设置为0.75倍的memory.mb，因为需要为Java code，非JVM内存使用等预留空间
	</describe>
	<describe>Haven't found the original description</describe>
</property>

<property>
        <name>mapreduce.reduce.java.opts</name>
        <value>-Xmx6g</value>
	<describe>见上</describe>
	<describe>Haven't found the original description</describe>
</property>




























